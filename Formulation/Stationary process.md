# Stationary process
In [mathematics](https://en.wikipedia.org/wiki/Mathematics "Mathematics") and [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), a **stationary process** (or a **strict/strictly stationary process** or **strong/strongly stationary process**) is a [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process "Stochastic process") whose unconditional [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution "Joint probability distribution") does not change when shifted in time.[\[1\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-1) Consequently, parameters such as [mean](https://en.wikipedia.org/wiki/Mean "Mean") and [variance](https://en.wikipedia.org/wiki/Variance "Variance") also do not change over time. To get an intuition of stationarity, one can imagine a [frictionless](https://en.wikipedia.org/wiki/Friction "Friction") [pendulum](https://en.wikipedia.org/wiki/Pendulum_(mechanics) "Pendulum (mechanics)"). It swings back and forth in an oscillatory motion, yet the [amplitude](https://en.wikipedia.org/wiki/Amplitude "Amplitude") and [frequency](https://en.wikipedia.org/wiki/Frequency "Frequency") remain constant. Although the pendulum is moving, the process is stationary as its "[statistics](https://en.wikipedia.org/wiki/Statistic "Statistic")" are constant (frequency and amplitude). However, if a [force](https://en.wikipedia.org/wiki/Force "Force") were to be applied to the pendulum (for example, friction with the air), either the frequency or amplitude would change, thus making the process _non-stationary._[\[2\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-2)

Since stationarity is an assumption underlying many statistical procedures used in [time series analysis](https://en.wikipedia.org/wiki/Time_series_analysis "Time series analysis"), non-stationary data are often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due either to the presence of a [unit root](https://en.wikipedia.org/wiki/Unit_root "Unit root") or of a deterministic trend. In the former case of a unit root, stochastic shocks have permanent effects, and the process is not [mean-reverting](https://en.wikipedia.org/wiki/Mean_reversion_(finance) "Mean reversion (finance)"). In the latter case of a deterministic trend, the process is called a [trend-stationary process](https://en.wikipedia.org/wiki/Trend-stationary_process "Trend-stationary process"), and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving (non-constant) mean.

A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a [cyclostationary process](https://en.wikipedia.org/wiki/Cyclostationary_process "Cyclostationary process"), which is a stochastic process that varies cyclically with time.

For many applications strict-sense stationarity is too restrictive. Other forms of stationarity such as **wide-sense stationarity** or **_N_\-th-order stationarity** are then employed. The definitions for different kinds of stationarity are not consistent among different authors (see [Other terminology](https://en.wikipedia.org/wiki/Stationary_process#Other_terminology "Stationary process")).

## Strict-sense stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=1 "Edit section: Strict-sense stationarity")\]

### Definition\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=2 "Edit section: Definition")\]

Formally, let ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) be a [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process "Stochastic process") and let ![{\displaystyle F_{X}(x_{t_{1}+\tau },\ldots ,x_{t_{n}+\tau })}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d6e739e9eb98b7bfabfa9afd7ba32e7fcafe63e5) represent the [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function "Cumulative distribution function") of the [unconditional](https://en.wikipedia.org/wiki/Marginal_distribution "Marginal distribution") (i.e., with no reference to any particular starting value) [joint distribution](https://en.wikipedia.org/wiki/Joint_distribution "Joint distribution") of ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) at times ![{\displaystyle t_{1}+\tau ,\ldots ,t_{n}+\tau }](https://wikimedia.org/api/rest_v1/media/math/render/svg/7f4a768913184f6318d9928686740f056505835a). Then, ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) is said to be **strictly stationary**, **strongly stationary** or **strict-sense stationary** if[\[3\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-KunIlPark-3):â€Šp. 155 

![{\displaystyle F_{X}(x_{t_{1}+\tau },\ldots ,x_{t_{n}+\tau })=F_{X}(x_{t_{1}},\ldots ,x_{t_{n}})\quad {\text{for all }}\tau ,t_{1},\ldots ,t_{n}\in \mathbb {R} {\text{ and for all }}n\in \mathbb {N} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6019735db0d37331a5db31776d4f59187ad4c5d8)

**(Eq.1)**

Since ![\tau ](https://wikimedia.org/api/rest_v1/media/math/render/svg/38a7dcde9730ef0853809fefc18d88771f95206c) does not affect ![F_{X}(\cdot )](https://wikimedia.org/api/rest_v1/media/math/render/svg/2dea82252546be08db3a23fd370e1e1bf1ee4eef), ![F_{{X}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/062f285db773e329f6c270cb6b65fa076996c941) is not a function of time.

### Examples\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=3 "Edit section: Examples")\]

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/Stationarycomparison.png/390px-Stationarycomparison.png)](https://en.wikipedia.org/wiki/File:Stationarycomparison.png)

[White noise](https://en.wikipedia.org/wiki/White_noise "White noise") is the simplest example of a stationary process.

An example of a [discrete-time](https://en.wikipedia.org/wiki/Discrete-time_stochastic_process "Discrete-time stochastic process") stationary process where the sample space is also discrete (so that the random variable may take one of _N_ possible values) is a [Bernoulli scheme](https://en.wikipedia.org/wiki/Bernoulli_scheme "Bernoulli scheme"). Other examples of a discrete-time stationary process with continuous sample space include some [autoregressive](https://en.wikipedia.org/wiki/Autoregressive "Autoregressive") and [moving average](https://en.wikipedia.org/wiki/Moving_average_model "Moving average model") processes which are both subsets of the [autoregressive moving average model](https://en.wikipedia.org/wiki/Autoregressive_moving_average_model "Autoregressive moving average model"). Models with a non-trivial autoregressive component may be either stationary or non-stationary, depending on the parameter values, and important non-stationary special cases are where [unit roots](https://en.wikipedia.org/wiki/Unit_root "Unit root") exist in the model.

#### Example 1\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=4 "Edit section: Example 1")\]

Let ![Y](https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f) be any scalar [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable"), and define a time-series ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a), by

![X_{t}=Y\qquad {\text{ for all }}t.](https://wikimedia.org/api/rest_v1/media/math/render/svg/ccbd374bf4bc159c936a942229e51c778bf8a97d)

Then ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) is a stationary time series, for which realisations consist of a series of constant values, with a different constant value for each realisation. A [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers "Law of large numbers") does not apply on this case, as the limiting value of an average from a single realisation takes the random value determined by ![Y](https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f), rather than taking the [expected value](https://en.wikipedia.org/wiki/Expected_value "Expected value") of ![Y](https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f).

The time average of ![X_{t}](https://wikimedia.org/api/rest_v1/media/math/render/svg/82120d04dfb3cbadc4912951dd12b5568c9cd8f3) does not converge since the process is not [ergodic](https://en.wikipedia.org/wiki/Ergodic_process "Ergodic process").

#### Example 2\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=5 "Edit section: Example 2")\]

As a further example of a stationary process for which any single realisation has an apparently noise-free structure, let ![Y](https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f) have a [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous) "Uniform distribution (continuous)") on ![{\displaystyle (0,2\pi ]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/40396ab04b04f76f310202bb1de2819064d4bf95) and define the time series ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) by

![X_{t}=\cos(t+Y)\quad {\text{ for }}t\in {\mathbb  {R}}.](https://wikimedia.org/api/rest_v1/media/math/render/svg/08c3a3c8b48bc79215025411bd42afd674ffa4a7)

Then ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) is strictly stationary.

#### Example 3\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=6 "Edit section: Example 3")\]

Keep in mind that a [white noise](https://en.wikipedia.org/wiki/White_noise "White noise") is not necessarily strictly stationary. Let ![\omega ](https://wikimedia.org/api/rest_v1/media/math/render/svg/48eff443f9de7a985bb94ca3bde20813ea737be8) be a random variable uniformly distributed in the interval ![{\displaystyle (0,2\pi )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8998332bd35fa470d4330cbc010e9329faadc096) and define the time series ![{\displaystyle \left\{z_{t}\right\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/224085bedfb9ac6b85cf26d469494df4354796c3)

![{\displaystyle z_{t}=\cos(t\omega )\quad (t=1,2,...)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/277bf43962ab8ed8118bd522d1d8ef1c72196302)

Then

![{\displaystyle {\begin{aligned}\mathbb {E} (z_{t})&={\frac {1}{2\pi }}\int _{0}^{2\pi }\cos(t\omega )d\omega =0\\Var(z_{t})&={\frac {1}{2\pi }}\int _{0}^{2\pi }\cos ^{2}(t\omega )d\omega =1/2\\Cov(z_{t},z_{j})&={\frac {1}{2\pi }}\int _{0}^{2\pi }\cos(t\omega )\cos(j\omega )d\omega =0\quad \forall t\neq j\\\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1d061199cc57ab9b109a76a78a424ca7f3e9779f).

So ![{\displaystyle \left\{z_{t}\right\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/224085bedfb9ac6b85cf26d469494df4354796c3) is a white noise, however it is not strictly stationary.

## _N_th-order stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=7 "Edit section: Nth-order stationarity")\]

In **[Eq.1](https://en.wikipedia.org/wiki/Stationary_process#math_Eq.1)**, the distribution of ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) samples of the stochastic process must be equal to the distribution of the samples shifted in time _for all_ ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b). _N_\-th-order stationarity is a weaker form of stationarity where this is only requested for all ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) up to a certain order ![N](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3). A random process ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) is said to be **_N_\-th-order stationary** if:[\[3\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-KunIlPark-3):â€Šp. 152 

![{\displaystyle F_{X}(x_{t_{1}+\tau },\ldots ,x_{t_{n}+\tau })=F_{X}(x_{t_{1}},\ldots ,x_{t_{n}})\quad {\text{for all }}\tau ,t_{1},\ldots ,t_{n}\in \mathbb {R} {\text{ and for all }}n\in \{1,\ldots ,N\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/35928d685091c85761dcfc4008b8609aacb7129b)

**(Eq.2)**

## Weak or wide-sense stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=8 "Edit section: Weak or wide-sense stationarity")\]

### Definition\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=9 "Edit section: Definition")\]

A weaker form of stationarity commonly employed in [signal processing](https://en.wikipedia.org/wiki/Signal_processing "Signal processing") is known as **weak-sense stationarity**, **wide-sense stationarity** (WSS), or **covariance stationarity**. WSS random processes only require that 1st [moment](https://en.wikipedia.org/wiki/Moment_(mathematics) "Moment (mathematics)") (i.e. the mean) and [autocovariance](https://en.wikipedia.org/wiki/Autocovariance "Autocovariance") do not vary with respect to time and that the 2nd moment is finite for all times. Any strictly stationary process which has a finite [mean](https://en.wikipedia.org/wiki/Mean "Mean") and a [covariance](https://en.wikipedia.org/wiki/Covariance "Covariance") is also WSS.[\[4\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-Florescu2014-4):â€Šp. 299 

So, a [continuous time](https://en.wikipedia.org/wiki/Continuous_time "Continuous time") [random process](https://en.wikipedia.org/wiki/Random_process "Random process") ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) which is WSS has the following restrictions on its mean function ![{\displaystyle m_{X}(t)\triangleq \operatorname {E} [X_{t}]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9a5d477bb97b0dc4f2ef23e4b59af4884a7785d2) and [autocovariance](https://en.wikipedia.org/wiki/Autocovariance "Autocovariance") function ![{\displaystyle K_{XX}(t_{1},t_{2})\triangleq \operatorname {E} [(X_{t_{1}}-m_{X}(t_{1}))(X_{t_{2}}-m_{X}(t_{2}))]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/489a454aa0134ec5cac24aefb7dcafe598146635):

![{\displaystyle {\begin{aligned}&m_{X}(t)=m_{X}(t+\tau )&&{\text{for all }}\tau \in \mathbb {R} \\&K_{XX}(t_{1},t_{2})=K_{XX}(t_{1}-t_{2},0)&&{\text{for all }}t_{1},t_{2}\in \mathbb {R} \\&\operatorname {E} [|X(t)|^{2}]<\infty &&{\text{for all }}t\in \mathbb {R} \end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4029b2926e1d56dfbda5e9b2cef503792f28dbf)

**(Eq.3)**

The first property implies that the mean function ![{\displaystyle m_{X}(t)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/417a96c8dc2b3a33eca58867e6b60f1902868ba1) must be constant. The second property implies that the covariance function depends only on the _difference_ between ![t_{1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cb0768c0bd659f2f84fb5ef9f4b74f336123d915) and ![t_{2}](https://wikimedia.org/api/rest_v1/media/math/render/svg/749fee708b41e7079eabd50d61c8bf3e965db16f) and only needs to be indexed by one variable rather than two variables.[\[3\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-KunIlPark-3):â€Šp. 159  Thus, instead of writing,

![{\displaystyle \,\!K_{XX}(t_{1}-t_{2},0)\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/eedb9be07f2c7c9636f1c19cc7195c8639bc064d)

the notation is often abbreviated by the substitution ![\tau =t_{1}-t_{2}](https://wikimedia.org/api/rest_v1/media/math/render/svg/42a12ecb05eb2d9c6278296ddf451169a7349361):

![{\displaystyle K_{XX}(\tau )\triangleq K_{XX}(t_{1}-t_{2},0)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/dec194bd73464d8f7bcb0ee38085ac25886031e4)

This also implies that the [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation "Autocorrelation") depends only on ![\tau =t_{1}-t_{2}](https://wikimedia.org/api/rest_v1/media/math/render/svg/42a12ecb05eb2d9c6278296ddf451169a7349361), that is

![{\displaystyle \,\!R_{X}(t_{1},t_{2})=R_{X}(t_{1}-t_{2},0)\triangleq R_{X}(\tau ).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d7c34316c92e031b5024417c458429e2f4f9cae1)

The third property says that the second moments must be finite for any time ![t](https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560).

### Motivation\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=10 "Edit section: Motivation")\]

The main advantage of wide-sense stationarity is that it places the time-series in the context of [Hilbert spaces](https://en.wikipedia.org/wiki/Hilbert_space "Hilbert space"). Let _H_ be the Hilbert space generated by {_x_(_t_)} (that is, the closure of the set of all linear combinations of these random variables in the Hilbert space of all square-integrable random variables on the given probability space). By the positive definiteness of the autocovariance function, it follows from [Bochner's theorem](https://en.wikipedia.org/wiki/Bochner%27s_theorem "Bochner's theorem") that there exists a positive measure ![\mu ](https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161) on the real line such that _H_ is isomorphic to the Hilbert subspace of _L_2(_Î¼_) generated by {_e_âˆ’2Ï€_iÎ¾â‹…t_}. This then gives the following Fourier-type decomposition for a continuous time stationary stochastic process: there exists a stochastic process ![{\displaystyle \omega _{\xi }}](https://wikimedia.org/api/rest_v1/media/math/render/svg/740b6e540611455d8fdffe389d809b718992184b) with [orthogonal increments](https://en.wikipedia.org/w/index.php?title=Orthogonal_increments&action=edit&redlink=1 "Orthogonal increments (page does not exist)") such that, for all ![t](https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560)

![{\displaystyle X_{t}=\int e^{-2\pi i\lambda \cdot t}\,d\omega _{\lambda },}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1eb3cc00a7e576f3ca53d2dbd869866b4403a499)

where the integral on the right-hand side is interpreted in a suitable (Riemann) sense. The same result holds for a discrete-time stationary process, with the spectral measure now defined on the unit circle.

When processing WSS random signals with [linear](https://en.wikipedia.org/wiki/Linear "Linear"), [time-invariant](https://en.wikipedia.org/wiki/Time-invariant "Time-invariant") ([LTI](https://en.wikipedia.org/wiki/LTI_system_theory "LTI system theory")) [filters](https://en.wikipedia.org/wiki/Filter_(signal_processing) "Filter (signal processing)"), it is helpful to think of the correlation function as a [linear operator](https://en.wikipedia.org/wiki/Linear_operator "Linear operator"). Since it is a [circulant](https://en.wikipedia.org/wiki/Circulant_matrix "Circulant matrix") operator (depends only on the difference between the two arguments), its eigenfunctions are the [Fourier](https://en.wikipedia.org/wiki/Fourier_series "Fourier series") complex exponentials. Additionally, since the [eigenfunctions](https://en.wikipedia.org/wiki/Eigenfunction "Eigenfunction") of LTI operators are also [complex exponentials](https://en.wikipedia.org/wiki/Exponential_function "Exponential function"), LTI processing of WSS random signals is highly tractableâ€”all computations can be performed in the [frequency domain](https://en.wikipedia.org/wiki/Frequency_domain "Frequency domain"). Thus, the WSS assumption is widely employed in signal processing [algorithms](https://en.wikipedia.org/wiki/Algorithm "Algorithm").

### Definition for complex stochastic process\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=11 "Edit section: Definition for complex stochastic process")\]

In the case where ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) is a complex stochastic process the [autocovariance](https://en.wikipedia.org/wiki/Autocovariance "Autocovariance") function is defined as ![{\displaystyle K_{XX}(t_{1},t_{2})=\operatorname {E} [(X_{t_{1}}-m_{X}(t_{1})){\overline {(X_{t_{2}}-m_{X}(t_{2}))}}]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cb49eacd4e53cf7ff63008bd07fea1b59ceee1de) and, in addition to the requirements in **[Eq.3](https://en.wikipedia.org/wiki/Stationary_process#math_Eq.3)**, it is required that the pseudo-autocovariance function ![{\displaystyle J_{XX}(t_{1},t_{2})=\operatorname {E} [(X_{t_{1}}-m_{X}(t_{1}))(X_{t_{2}}-m_{X}(t_{2}))]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8d0de9e9803217e228b158221440705cacc738b) depends only on the time lag. In formulas, ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) is WSS, if

![{\displaystyle {\begin{aligned}&m_{X}(t)=m_{X}(t+\tau )&&{\text{for all }}\tau \in \mathbb {R} \\&K_{XX}(t_{1},t_{2})=K_{XX}(t_{1}-t_{2},0)&&{\text{for all }}t_{1},t_{2}\in \mathbb {R} \\&J_{XX}(t_{1},t_{2})=J_{XX}(t_{1}-t_{2},0)&&{\text{for all }}t_{1},t_{2}\in \mathbb {R} \\&\operatorname {E} [|X(t)|^{2}]<\infty &&{\text{for all }}t\in \mathbb {R} \end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/137de70322fa0e9b67ed35ddae3c54916393c7ad)

**(Eq.4)**

## Joint stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=12 "Edit section: Joint stationarity")\]

The concept of stationarity may be extended to two stochastic processes.

### Joint strict-sense stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=13 "Edit section: Joint strict-sense stationarity")\]

Two stochastic processes ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) and ![{\displaystyle \left\{Y_{t}\right\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0f6c45b3e23284b76bf9081ac85c1d518ea6fca) are called **jointly strict-sense stationary** if their joint cumulative distribution ![{\displaystyle F_{XY}(x_{t_{1}},\ldots ,x_{t_{m}},y_{t_{1}^{'}},\ldots ,y_{t_{n}^{'}})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3a061637bdb998eeff3ee842edc3946005a66b15) remains unchanged under time shifts, i.e. if

![{\displaystyle F_{XY}(x_{t_{1}},\ldots ,x_{t_{m}},y_{t_{1}^{'}},\ldots ,y_{t_{n}^{'}})=F_{XY}(x_{t_{1}+\tau },\ldots ,x_{t_{m}+\tau },y_{t_{1}^{'}+\tau },\ldots ,y_{t_{n}^{'}+\tau })\quad {\text{for all }}\tau ,t_{1},\ldots ,t_{m},t_{1}^{'},\ldots ,t_{n}^{'}\in \mathbb {R} {\text{ and for all }}m,n\in \mathbb {N} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/25544004b368839cbf0d5029feea99ad3988af8e)

**(Eq.5)**

### Joint (_M_ + _N_)th-order stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=14 "Edit section: Joint (M + N)th-order stationarity")\]

Two random processes ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) and ![{\displaystyle \left\{Y_{t}\right\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0f6c45b3e23284b76bf9081ac85c1d518ea6fca) is said to be **jointly (_M_Â +Â _N_)-th-order stationary** if:[\[3\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-KunIlPark-3):â€Šp. 159 

![{\displaystyle F_{XY}(x_{t_{1}},\ldots ,x_{t_{m}},y_{t_{1}^{'}},\ldots ,y_{t_{n}^{'}})=F_{XY}(x_{t_{1}+\tau },\ldots ,x_{t_{m}+\tau },y_{t_{1}^{'}+\tau },\ldots ,y_{t_{n}^{'}+\tau })\quad {\text{for all }}\tau ,t_{1},\ldots ,t_{m},t_{1}^{'},\ldots ,t_{n}^{'}\in \mathbb {R} {\text{ and for all }}m\in \{1,\ldots ,M\},n\in \{1,\ldots ,N\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4c084135a9903306f04eac6421b820e8fcd261ac)

**(Eq.6)**

### Joint weak or wide-sense stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=15 "Edit section: Joint weak or wide-sense stationarity")\]

Two stochastic processes ![\left\{X_{t}\right\}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7d06db272f40cad57e421e3a38af88597d0709a) and ![{\displaystyle \left\{Y_{t}\right\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0f6c45b3e23284b76bf9081ac85c1d518ea6fca) are called **jointly wide-sense stationary** if they are both wide-sense stationary and their cross-covariance function ![{\displaystyle K_{XY}(t_{1},t_{2})=\operatorname {E} [(X_{t_{1}}-m_{X}(t_{1}))(Y_{t_{2}}-m_{Y}(t_{2}))]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/184c3b999b79901982ed86a559cc5ab4209b72a0) depends only on the time difference ![\tau =t_{1}-t_{2}](https://wikimedia.org/api/rest_v1/media/math/render/svg/42a12ecb05eb2d9c6278296ddf451169a7349361). This may be summarized as follows:

![{\displaystyle {\begin{aligned}&m_{X}(t)=m_{X}(t+\tau )&&{\text{for all }}\tau \in \mathbb {R} \\&m_{Y}(t)=m_{Y}(t+\tau )&&{\text{for all }}\tau \in \mathbb {R} \\&K_{XX}(t_{1},t_{2})=K_{XX}(t_{1}-t_{2},0)&&{\text{for all }}t_{1},t_{2}\in \mathbb {R} \\&K_{YY}(t_{1},t_{2})=K_{YY}(t_{1}-t_{2},0)&&{\text{for all }}t_{1},t_{2}\in \mathbb {R} \\&K_{XY}(t_{1},t_{2})=K_{XY}(t_{1}-t_{2},0)&&{\text{for all }}t_{1},t_{2}\in \mathbb {R} \end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/65ea04281c1c17c9327e02d3c174def58f1440a8)

**(Eq.7)**

## Relation between types of stationarity\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=16 "Edit section: Relation between types of stationarity")\]

## Other terminology\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=17 "Edit section: Other terminology")\]

The terminology used for types of stationarity other than strict stationarity can be rather mixed. Some examples follow.

-   [Priestley](https://en.wikipedia.org/wiki/Maurice_Priestley "Maurice Priestley") uses **stationary up to order** _m_ if conditions similar to those given here for wide sense stationarity apply relating to moments up to order _m_.[\[5\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-5)[\[6\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-6) Thus wide sense stationarity would be equivalent to "stationary to order 2", which is different from the definition of second-order stationarity given here.
-   [Honarkhah](https://en.wikipedia.org/w/index.php?title=Mehrdad_Honarkhah&action=edit&redlink=1 "Mehrdad Honarkhah (page does not exist)") and [Caers](https://en.wikipedia.org/wiki/Jef_Caers "Jef Caers") also use the assumption of stationarity in the context of multiple-point geostatistics, where higher n-point statistics are assumed to be stationary in the spatial domain.[\[7\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-7)
-   [Tahmasebi](https://en.wikipedia.org/w/index.php?title=Pejman_Tahmasebi&action=edit&redlink=1 "Pejman Tahmasebi (page does not exist)") and [Sahimi](https://en.wikipedia.org/wiki/Muhammad_Sahimi "Muhammad Sahimi") have presented an adaptive Shannon-based methodology that can be used for modeling of any non-stationary systems.[\[8\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-8)

## Differencing\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=18 "Edit section: Differencing")\]

One way to make some time series stationary is to compute the differences between consecutive observations. This is known as [differencing](https://en.wikipedia.org/wiki/Unit_root "Unit root"). Differencing can help stabilize the mean of a time series by removing changes in the level of a time series, and so eliminating trend and seasonality.

Transformations such as logarithms can help to stabilize the variance of a time series.

One of the ways for identifying non-stationary times series is the [ACF](https://en.wikipedia.org/wiki/Autocorrelation "Autocorrelation") plot. For a stationary time series, the [ACF](https://en.wikipedia.org/wiki/Autocorrelation "Autocorrelation") will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.[\[9\]](https://en.wikipedia.org/wiki/Stationary_process#cite_note-9)

## References\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=20 "Edit section: References")\]

1.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-1 "Jump up")** Gagniuc, Paul A. (2017). _Markov Chains: From Theory to Implementation and Experimentation_. USA, NJ: John Wiley & Sons. pp.Â 1â€“256. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")Â [978-1-119-38755-8](https://en.wikipedia.org/wiki/Special:BookSources/978-1-119-38755-8 "Special:BookSources/978-1-119-38755-8").
2.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-2 "Jump up")** Laumann, Timothy O.; Snyder, Abraham Z.; Mitra, Anish; Gordon, Evan M.; Gratton, Caterina; Adeyemo, Babatunde; Gilmore, Adrian W.; Nelson, Steven M.; Berg, Jeff J.; Greene, Deanna J.; McCarthy, John E. (2016-09-02). ["On the Stability of BOLD fMRI Correlations"](https://doi.org/10.1093/cercor/bhw265). _Cerebral Cortex_. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1093/cercor/bhw265](https://doi.org/10.1093%2Fcercor%2Fbhw265). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)")Â [1047-3211](https://www.worldcat.org/issn/1047-3211). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) "PMC (identifier)")Â [6248456](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6248456). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")Â [27591147](https://pubmed.ncbi.nlm.nih.gov/27591147).
3.  ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-0) [_**b**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-1) [_**c**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-2) [_**d**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-3) [_**e**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-4) [_**f**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-5) [_**g**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-KunIlPark_3-6) Park,Kun Il (2018). _Fundamentals of Probability and Stochastic Processes with Applications to Communications_. Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")Â [978-3-319-68074-3](https://en.wikipedia.org/wiki/Special:BookSources/978-3-319-68074-3 "Special:BookSources/978-3-319-68074-3").
4.  ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-Florescu2014_4-0) [_**b**_](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-Florescu2014_4-1) Ionut Florescu (7 November 2014). _Probability and Stochastic Processes_. John Wiley & Sons. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")Â [978-1-118-59320-2](https://en.wikipedia.org/wiki/Special:BookSources/978-1-118-59320-2 "Special:BookSources/978-1-118-59320-2").
5.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-5 "Jump up")** Priestley, M. B. (1981). _Spectral Analysis and Time Series_. Academic Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")Â [0-12-564922-3](https://en.wikipedia.org/wiki/Special:BookSources/0-12-564922-3 "Special:BookSources/0-12-564922-3").
6.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-6 "Jump up")** Priestley, M. B. (1988). [_Non-linear and Non-stationary Time Series Analysis_](https://archive.org/details/nonlinearnonstat0000prie). Academic Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")Â [0-12-564911-8](https://en.wikipedia.org/wiki/Special:BookSources/0-12-564911-8 "Special:BookSources/0-12-564911-8").
7.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-7 "Jump up")** Honarkhah, M.; Caers, J. (2010). "Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling". _Mathematical Geosciences_. **42** (5): 487â€“517. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1007/s11004-010-9276-7](https://doi.org/10.1007%2Fs11004-010-9276-7).
8.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-8 "Jump up")** Tahmasebi, P.; Sahimi, M. (2015). ["Reconstruction of nonstationary disordered materials and media: Watershed transform and cross-correlation function"](http://journals.aps.org/pre/abstract/10.1103/PhysRevE.91.032401) (PDF). _Physical Review E_. **91** (3): 032401. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1103/PhysRevE.91.032401](https://doi.org/10.1103%2FPhysRevE.91.032401). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")Â [25871117](https://pubmed.ncbi.nlm.nih.gov/25871117).
9.  **[^](https://en.wikipedia.org/wiki/Stationary_process#cite_ref-9 "Jump up")** ["8.1 Stationarity and differencing | OTexts"](https://www.otexts.org/fpp/8/1). _www.otexts.org_. Retrieved 2016-05-18.

## Further reading\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=21 "Edit section: Further reading")\]

-   Enders, Walter (2010). _Applied Econometric Time Series_ (ThirdÂ ed.). New York: Wiley. pp.Â 53â€“57. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)")Â [978-0-470-50539-7](https://en.wikipedia.org/wiki/Special:BookSources/978-0-470-50539-7 "Special:BookSources/978-0-470-50539-7").
-   Jestrovic, I.; Coyle, J. L.; Sejdic, E (2015). ["The effects of increased fluid viscosity on stationary characteristics of EEG signal in healthy adults"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253861). _Brain Research_. **1589**: 45â€“53. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/j.brainres.2014.09.035](https://doi.org/10.1016%2Fj.brainres.2014.09.035). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) "PMC (identifier)")Â [4253861](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253861). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)")Â [25245522](https://pubmed.ncbi.nlm.nih.gov/25245522).
-   Hyndman, Athanasopoulos (2013). Forecasting: Principles and Practice. Otexts. [https://www.otexts.org/fpp/8/1](https://www.otexts.org/fpp/8/1)

## External links\[[edit](https://en.wikipedia.org/w/index.php?title=Stationary_process&action=edit&section=22 "Edit section: External links")\]

-   [Spectral decomposition of a random function (Springer)](https://encyclopediaofmath.org/wiki/Spectral_decomposition_of_a_random_function)